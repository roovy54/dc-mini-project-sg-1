{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data"
      ],
      "metadata": {
        "id": "ZnZWKtsj29h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AJvLuub3BFa",
        "outputId": "90d51a8d-6dfb-4821-db9a-942f81551ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# loading the wmt14 dataset\n",
        "dataset = load_dataset(\"wmt14\", \"de-en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kefZKjXq3IUu",
        "outputId": "2734cd3c-ba90-4cc5-eca0-a64fbe10f4f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting into train, validation and test\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "val_dataset = dataset[\"validation\"]\n",
        "test_dataset = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "8RCSVJJ43mlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the sentences in list\n",
        "\n",
        "train_sentences_en = []\n",
        "train_sentences_de = []\n",
        "val_sentences_en = []\n",
        "val_sentences_de = []\n",
        "\n",
        "for i in range(50000): # only 50000 sentences are taken from train (out of 4 lakh samples in total)\n",
        "  train_sentences_en.append(train_dataset[i][\"translation\"]['en'])\n",
        "  train_sentences_de.append(train_dataset[i][\"translation\"]['de'])\n",
        "\n",
        "for i in range(len(val_dataset)):\n",
        "  val_sentences_en.append(val_dataset[i][\"translation\"]['en'])\n",
        "  val_sentences_de.append(val_dataset[i][\"translation\"]['de'])"
      ],
      "metadata": {
        "id": "zhErlI293RFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_sentences_en), len(train_sentences_de), len(val_sentences_en), len(val_sentences_de)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PihKTe2S5fB-",
        "outputId": "9262a1b0-8d41-4462-97e4-19e907b7faba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 50000, 3000, 3000)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Training Tokenizer"
      ],
      "metadata": {
        "id": "m8v5owLb6jA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# counting unique words in the sentences\n",
        "\n",
        "def count_unique_words(sentences):\n",
        "    words = set()\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.lower().split():\n",
        "            words.add(word.strip(\".,!?;:()[]\\\"'\"))  # remove common punctuations\n",
        "    return len(words)\n",
        "\n",
        "count_vocab_size = count_unique_words(train_sentences_en + train_sentences_de)\n",
        "\n",
        "count_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws3ZKpJc6v05",
        "outputId": "3b46e570-46f2-4dad-83bf-64cb3432e83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69447"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.normalizers import Sequence, Lowercase, NFD, StripAccents\n",
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "import random\n",
        "\n",
        "def train_bpe_tokenizer_from_sentences(sentences, vocab_size=100000):\n",
        "    # Initialize a tokenizer with the BPE model\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "\n",
        "    # Set normalization and pre-tokenization\n",
        "    tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "    # Set up the trainer\n",
        "    trainer = BpeTrainer(\n",
        "        vocab_size=vocab_size,\n",
        "        special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "    )\n",
        "\n",
        "    # Train using the list of sentences\n",
        "    tokenizer.train_from_iterator(sentences, trainer=trainer)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "sentences = train_sentences_de + train_sentences_en\n",
        "random.shuffle(sentences)\n",
        "\n",
        "# Train tokenizer\n",
        "tokenizer = train_bpe_tokenizer_from_sentences(sentences, vocab_size=100000)"
      ],
      "metadata": {
        "id": "a0snqs2S7p3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode a english test sentence\n",
        "output = tokenizer.encode(\"Tokenizers are awesome!\")\n",
        "print(\"Tokens:\", output.tokens)\n",
        "\n",
        "# Encode a german test sentence\n",
        "output = tokenizer.encode(\"Die Katze sitzt auf dem Fensterbrett und schaut nach draußen.\")\n",
        "print(\"Tokens:\", output.tokens, \"IDs:\", output.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPWTOgY78qb_",
        "outputId": "f4dfb7d3-d884-478c-f296-eafffcd4d710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['token', 'iz', 'ers', 'are', 'a', 'wes', 'ome', '!']\n",
            "Tokens: ['die', 'katze', 'sitzt', 'auf', 'dem', 'fenster', 'brett', 'und', 'schaut', 'nach', 'draußen', '.'] IDs: [101, 65572, 16242, 164, 234, 25763, 24442, 110, 34732, 340, 20471, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model"
      ],
      "metadata": {
        "id": "FEIpq2_S-BxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "# Transformer with tokenizer integration\n",
        "class TransformerTranslationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, tokenizer,\n",
        "                 d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load tokenizers\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.src_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        self.pos_decoder = PositionalEncoding(d_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.tgt_vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None, memory_key_padding_mask=None):\n",
        "        src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n",
        "        tgt_emb = self.pos_decoder(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n",
        "\n",
        "        output = self.transformer(\n",
        "            src_emb, tgt_emb,\n",
        "            src_mask=src_mask, tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "        return self.fc_out(output)\n",
        "\n",
        "    def encode_input(self, src_text):\n",
        "        # Encode input text using source tokenizer\n",
        "        return torch.tensor(self.tokenizer.encode(src_text).ids).unsqueeze(1)  # (seq_len, 1)\n",
        "\n",
        "    def decode_output(self, token_ids):\n",
        "        return self.tokenizer.decode(token_ids, skip_special_tokens=True)\n",
        "\n",
        "    def generate(self, src_text, max_len=50):\n",
        "        self.eval()\n",
        "        src = self.encode_input(src_text).to(next(self.parameters()).device)  # (src_len, 1)\n",
        "\n",
        "        tgt_tokens = [self.tokenizer.token_to_id(\"[CLS]\")]\n",
        "        for _ in range(max_len):\n",
        "            tgt_input = torch.tensor(tgt_tokens).unsqueeze(1).to(src.device)  # (tgt_len, 1)\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(src.device)\n",
        "\n",
        "            out = self.forward(src, tgt_input, tgt_mask=tgt_mask)\n",
        "            next_token = out.argmax(dim=-1)[-1, 0].item()\n",
        "\n",
        "            if next_token == self.tgt_tokenizer.token_to_id(\"[SEP]\"):\n",
        "                break\n",
        "\n",
        "            tgt_tokens.append(next_token)\n",
        "\n",
        "        return self.decode_output(tgt_tokens[1:])  # skip [CLS]"
      ],
      "metadata": {
        "id": "sWwAL8Kf9dk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Pytorch Dataset"
      ],
      "metadata": {
        "id": "wtnTqkbB5p7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- Custom Dataset ---\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, tokenizer, max_len=128):\n",
        "        self.src = src_sentences\n",
        "        self.tgt = tgt_sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
        "        self.cls_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "        self.sep_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_encoded = self.tokenizer.encode(self.src[idx]).ids\n",
        "        tgt_encoded = self.tokenizer.encode(self.tgt[idx]).ids\n",
        "\n",
        "        src_ids = src_encoded[:self.max_len]\n",
        "        tgt_ids = [self.cls_id] + tgt_encoded[:self.max_len - 2] + [self.sep_id]\n",
        "\n",
        "        # Padding\n",
        "        src_ids += [self.pad_id] * (self.max_len - len(src_ids))\n",
        "        tgt_ids += [self.pad_id] * (self.max_len - len(tgt_ids))\n",
        "\n",
        "        src = torch.tensor(src_ids)\n",
        "        tgt = torch.tensor(tgt_ids)\n",
        "        return src, tgt\n",
        "\n",
        "train_dataset = TranslationDataset(train_sentences_en, train_sentences_de, tokenizer)\n",
        "val_dataset = TranslationDataset(val_sentences_en, val_sentences_de, tokenizer)"
      ],
      "metadata": {
        "id": "TDeEXJL75s4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Pytorch Dataloader"
      ],
      "metadata": {
        "id": "WkODS2X96dBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "H7PgYcUZ6h2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2yYRDW3F5rS",
        "outputId": "ed757e8b-37a5-466f-bd10-c47c28a77e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 112,  759, 1092,  ...,    0,    0,    0],\n",
              "        [5773,   14,  526,  ...,    0,    0,    0],\n",
              "        [  82,  753, 5345,  ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [  82, 3107,   79,  ...,    0,    0,    0],\n",
              "        [ 112,  401,   14,  ...,    0,    0,    0],\n",
              "        [ 127,   33, 4156,  ...,    0,    0,    0]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Training"
      ],
      "metadata": {
        "id": "jkB8v9iOAzEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Generate Padding Mask ---\n",
        "def create_padding_mask(seq, pad_id):\n",
        "    return (seq == pad_id)\n",
        "\n",
        "def train_model(model, dataloader, tokenizer, epochs=10, lr=1e-4, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id(\"[PAD]\"))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "        for src, tgt in loop:\n",
        "            src = src.to(device)         # (batch, seq_len)\n",
        "            tgt = tgt.to(device)         # (batch, seq_len)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]      # (batch, seq_len - 1)\n",
        "            tgt_output = tgt[:, 1:]      # (batch, seq_len - 1)\n",
        "\n",
        "            # Transpose for transformer (seq_len, batch)\n",
        "            src = src.transpose(0, 1)\n",
        "            tgt_input = tgt_input.transpose(0, 1)\n",
        "\n",
        "            # Masks\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(device)\n",
        "            pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
        "            src_padding_mask = create_padding_mask(src.transpose(0, 1), pad_id).to(device)\n",
        "            tgt_padding_mask = create_padding_mask(tgt_input.transpose(0, 1), pad_id).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(\n",
        "                src, tgt_input,\n",
        "                src_mask=None, tgt_mask=tgt_mask,\n",
        "                src_padding_mask=src_padding_mask,\n",
        "                tgt_padding_mask=tgt_padding_mask,\n",
        "                memory_key_padding_mask=src_padding_mask\n",
        "            )\n",
        "\n",
        "            output = output.transpose(0, 1).contiguous()  # (batch, seq_len, vocab_size)\n",
        "            loss = criterion(output.view(-1, output.size(-1)), tgt_output.reshape(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Average Loss = {total_loss / len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "xFYbzJjXBN6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "model = TransformerTranslationModel(vocab_size, tokenizer)\n",
        "\n",
        "# Train\n",
        "train_model(model, train_loader, tokenizer, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4v4aK0jCgrN",
        "outputId": "910ec5ce-04be-49d1-be3a-c116de16a843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "Epoch 1:   0%|          | 0/1563 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "                                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Average Loss = 5.9674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Testing on a example sentence"
      ],
      "metadata": {
        "id": "QlxMUjdBLyvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, tokenizer, sentence, device='cuda', max_len=50):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Encode source sentence\n",
        "    src_tensor = model.encode_input(sentence).to(device)  # (src_len, 1)\n",
        "\n",
        "    # Start decoding with [CLS] token\n",
        "    tgt_tokens = [tokenizer.token_to_id(\"[CLS]\")]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        tgt_input = torch.tensor(tgt_tokens).unsqueeze(1).to(device)  # (tgt_len, 1)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(\n",
        "                src_tensor, tgt_input,\n",
        "                src_mask=None, tgt_mask=tgt_mask,\n",
        "                src_padding_mask=None,\n",
        "                tgt_padding_mask=None,\n",
        "                memory_key_padding_mask=None\n",
        "            )\n",
        "\n",
        "        next_token = output.argmax(dim=-1)[-1, 0].item()\n",
        "        if next_token == tokenizer.token_to_id(\"[SEP]\"):\n",
        "            break\n",
        "\n",
        "        tgt_tokens.append(next_token)\n",
        "\n",
        "    # Decode the output tokens (skipping [CLS])\n",
        "    translation = model.decode_output(tgt_tokens[1:])\n",
        "    return translation"
      ],
      "metadata": {
        "id": "4kO13sy8M1wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = \"lion\"\n",
        "translation = translate_sentence(model, tokenizer, example_sentence)\n",
        "print(\"Translation:\", translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAqvIAozM2x8",
        "outputId": "f9853a14-098e-484e-9f8c-8a7c5f66736a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation: ( das parlament )\n"
          ]
        }
      ]
    }
  ]
}