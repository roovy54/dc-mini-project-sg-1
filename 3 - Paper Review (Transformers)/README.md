# Task 3: Summary of "Attention is All You Need" Paper

## Overview
This repository contains a summary document of the paper **"Attention is All You Need"** by Vaswani et al. (2017). The paper introduces the Transformer model, a novel architecture that relies entirely on attention mechanisms, eliminating the need for recurrent or convolutional layers.

The summary document is a 2-3 page explanation of the key concepts, insights, and contributions of the paper, written in a standard font size of 11.

---

## Resources for Understanding the Paper
The following resources were used to better understand the paper:
1. [The Transformer Model - A Step-by-Step Breakdown](https://youtu.be/wjZofJX0v4M?si=Z-ddbt7eNlTb2z-v)
2. [Attention Mechanism in Deep Learning](https://youtu.be/eMlx5fFNoYc?si=H3impge5fNPloD_W)
3. [Transformers Explained Visually](https://youtu.be/9-Jl0dxWQs8?si=Zfnjm_0i5kAYp1-D)
4. [The Illustrated Transformer](https://www.youtube.com/watch?v=zxQyTK8quyY)

---
